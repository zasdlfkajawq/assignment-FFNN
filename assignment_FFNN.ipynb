{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whd-5aC-vVEj"
      },
      "source": [
        "# Assignment 3: Build a Feedforward Neural Net for Sentiment Classification (100 Points)\n",
        "\n",
        "Instructor: Ziyu Yao; Class: CS478 Fall 2024\n",
        "\n",
        "Release on Sept 18, 2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3sB2t8gvVEk"
      },
      "source": [
        "**Assignment Overview:** This assignment will guide you to use the PyTorch library (https://pytorch.org/) to construct a feedforward neural net (FFNN) for sentiment classification. We will use the movie review dataset of Socher et al. (2013). The original dataset has fine-grained sentiment labels (e.g., highly negative vs. negative), but we will consider a simplified task version with only two class labels, i.e., positive (1) and negative (0). Therefore, this will be a binary classification task.\n",
        "\n",
        "This assignment consists of six parts and shoud be submitted in three checkpoints (check out the PDF for instructions):\n",
        "\n",
        "_Checkpoint 1, from Part 1 to Part 2 (Due on Sept 30)_\n",
        "- **Part 1 (2 Points):** Get to know PyTorch;\n",
        "- **Part 2 (3 Points):** Data structure and loading;\n",
        "\n",
        "_Checkpoint 2, from Part 3 to Part 4 (Due on Oct 9)_\n",
        "- **Part 3 (5 Points):** FFNN model construction;\n",
        "- **Part 4 (3 Points):** Sentiment classifier training and evaluation;\n",
        "\n",
        "_Checkpoint 3, from Part 5 to Part 6 (Due on Oct 16)_\n",
        "- **Part 5 (4 Points):** Exploration of FFNN hyper-parameters;\n",
        "- **Part 6 (3 Points):** Exploration of the pre-trained GloVe word embedding.\n",
        "\n",
        "**Tutorial for Beginners:** If you are new to PyTorch, you are highly recommended to watch the tutorial: https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7X4zatrx7NV"
      },
      "source": [
        "## Part 0: PyTorch Installation\n",
        "Install the PyTorch library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DFOJ3HdwyQ3W"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in c:\\users\\17034\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\17034\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\17034\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\17034\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in c:\\users\\17034\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\17034\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\17034\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\17034\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (75.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\17034\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\17034\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9sqEzwUvVEl"
      },
      "source": [
        "## Part 1: Get to Know PyTorch (10 Points)\n",
        "\n",
        "To help you kick start this assignment, here are some small practices about PyTorch basics.\n",
        "\n",
        "Let's first import the PyTorch library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in c:\\users\\17034\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.1.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CuVDR35pvVEl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-98qOuIvVEl"
      },
      "source": [
        "Consider two 2D tensors, $\\mathrm{a}$ and $\\mathrm{b}$, which are parameters of a neural network (NN). The NN is defined as $\\mathrm{f}(\\mathrm{a}, \\mathrm{b}) = 3\\mathrm{a}^3 - \\mathrm{b}^2$.\n",
        "\n",
        "All operations here are element-wise. Given the 2D tensors, the function can be expressed equivalently in the following form:\n",
        "$$\n",
        "\\begin{pmatrix} f_0 \\\\ f_1 \\end{pmatrix} = 3 \\begin{pmatrix} a_0^3 \\\\ a_1^3 \\end{pmatrix} - \\begin{pmatrix} b_0^2 \\\\ b_1^2 \\end{pmatrix}\n",
        "$$\n",
        "where\n",
        "$$\n",
        "\\mathrm{a} = \\begin{pmatrix} a_0 \\\\ a_1 \\end{pmatrix}, \\: \\mathrm{b} = \\begin{pmatrix} b_0 \\\\ b_1 \\end{pmatrix}, \\: \\mathrm{f} = \\begin{pmatrix} f_0 \\\\ f_1 \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "We also define a loss function $l = f_0 + 2 f_1$. Note that $l$ is a scalar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPq7aXMXvVEm"
      },
      "source": [
        "Q1: Now given the following two tensors $\\mathrm{a}$ and $\\mathrm{b}$ (`requires_grad=True` for tracking their gradients),"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pcv39Kl2vVEm"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-_T9kedvVEm"
      },
      "source": [
        "can you implement the forward pass of the neural network function and its loss, and show their values?\n",
        "\n",
        "<font color='blue'>YOUR TASK: Please complete the following code block and answer the question.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pveOnYu8vVEm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f: tensor([-12.,  65.], grad_fn=<SubBackward0>)\n",
            "l: 118\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "f = 3 * (a**3) - (b**2)\n",
        "l = (-12) + (2*65)\n",
        "\n",
        "print(\"f:\", f)\n",
        "print(\"l:\", l)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwG_q7TzvVEm"
      },
      "source": [
        "Q2: Given the values of `a` and `b` and the loss `l`, what will be the gradients of them?\n",
        "\n",
        "<font color='blue'>YOUR TASK: Please write down the formula of each partial derivative and complete the calculation below. The example of $\\frac{\\partial l}{\\partial a_0}$ is given.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-O_xpuIvVEn"
      },
      "source": [
        "$\\frac{\\partial l}{\\partial a_0} = 9 a_0^2 = 36$, $\\frac{\\partial l}{\\partial a_1} = 9 a_1^2 = 81$, $\\frac{\\partial l}{\\partial b_0} = 2 b_0 = 12$, $\\frac{\\partial l}{\\partial b_1} = 2 b_1 = 8$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNzATxzavVEn"
      },
      "source": [
        "Q3: Now, try to use PyTorch's `autograd` to calculate the gradients automatically.\n",
        "\n",
        "<font color='blue'>YOUR TASK: Please complete the following code block and answer the question.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oHB8ZahtvVEn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gradient of `a`: tensor([ 36., 486.])\n",
            "gradient of `b`: tensor([-12.,  -8.])\n"
          ]
        }
      ],
      "source": [
        "# TODO: apply the autograd function to the loss `l` to calculate the gradients of `a` and `b`\n",
        "loss = (((3*a[0]**3)) - (b[0]**2)) + (2*(3 * (3*a[1]**3)) - (b[1]**2))\n",
        "loss.backward()\n",
        "print(\"gradient of `a`:\", a.grad)\n",
        "print(\"gradient of `b`:\", b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNuj18h8vVEn"
      },
      "source": [
        "_If your answers to Q2 and Q3 are both correct, you should exactly the same calculation results from them._\n",
        "\n",
        "**Up to now, you've gained some sense about PyTorch. From Part 2, we will start building the Feedforward Neural Net (FFNN) for sentiment classification. Before it, let's load the following libraries.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AaHwxhDIvVEn"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import os\n",
        "\n",
        "# Set up overall seed\n",
        "seed = 12345\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QwNXrznvVEn"
      },
      "source": [
        "## Part 2: Data Structure and Loading (15 Points)\n",
        "\n",
        "**Step 1:** We will first define the data structure for storing the sentiment data. _There's nothing to fill out from your side; however, you should try to understand the data implementation._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "03k9RSzivVEn"
      },
      "outputs": [],
      "source": [
        "class SentimentExample:\n",
        "    \"\"\"\n",
        "    Data wrapper for a single example for sentiment analysis.\n",
        "\n",
        "    Attributes:\n",
        "        words (List[string]): list of words\n",
        "        label (int): 0 or 1 (0 = negative, 1 = positive)\n",
        "        word_indices (List[int]): list of word indices in the vocab, which will generated by the `indexing_sentiment_examples` method\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, words, label):\n",
        "        self.words = words\n",
        "        self.label = label\n",
        "        self.word_indices = None # the word indices in vocab\n",
        "\n",
        "    def __repr__(self):\n",
        "        return repr(self.words) + \"; label=\" + repr(self.label)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThgLDe-7vVEn"
      },
      "source": [
        "Essentially, this `SentimentExample` class defines each data example in a sentiment dataset to have three attributes: the word tokens in the sentence, the sentiment label (1 or 0), and the word indices in the vocabulary (which has not been defined yet).\n",
        "\n",
        "To assist the data loading the writing, we also define the following help functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-TyPEzGJvVEn"
      },
      "outputs": [],
      "source": [
        "def indexing_sentiment_examples(exs: List[SentimentExample], vocabulary: List[str], UNK_idx: int):\n",
        "    \"\"\"\n",
        "    Indexing words in each SentimentExample based on a given vocabulary. This method will directly modify the `word_indices` attribute of each ex.\n",
        "    :param exs: a list of SentimentExample objects\n",
        "    :param vocabulary: the vocabulary, which should be a list of words\n",
        "    :param UNK_idx: the index of UNK token in the vocabulary\n",
        "    \"\"\"\n",
        "    for ex in exs:\n",
        "        ex.word_indices = [vocabulary.index(word) if word in vocabulary else UNK_idx for word in ex.words]\n",
        "\n",
        "def read_sentiment_examples(infile: str) -> List[SentimentExample]:\n",
        "    \"\"\"\n",
        "    Reads sentiment examples in the format [0 or 1]<TAB>[raw sentence]; tokenizes and cleans the sentences and forms\n",
        "    SentimentExamples. Note that all words have been lowercased.\n",
        "\n",
        "    :param infile: file to read from\n",
        "    :return: a list of SentimentExamples parsed from the file\n",
        "    \"\"\"\n",
        "    f = open(infile)\n",
        "    exs = []\n",
        "    for line in f:\n",
        "        if len(line.strip()) > 0:\n",
        "            line = line.strip()\n",
        "            fields = line.split(\"\\t\")\n",
        "            if len(fields) != 2:\n",
        "                fields = line.split()\n",
        "                label = 0 if \"0\" in fields[0] else 1\n",
        "                sent = \" \".join(fields[1:])\n",
        "            else:\n",
        "                # Slightly more robust to reading bad output than int(fields[0])\n",
        "                label = 0 if \"0\" in fields[0] else 1\n",
        "                sent = fields[1]\n",
        "            sent = sent.lower() # lowercasing\n",
        "            tokenized_cleaned_sent = list(filter(lambda x: x != '', sent.rstrip().split(\" \")))\n",
        "            exs.append(SentimentExample(tokenized_cleaned_sent, label))\n",
        "    f.close()\n",
        "    return exs\n",
        "\n",
        "\n",
        "def read_blind_sst_examples(infile: str) -> List[SentimentExample]:\n",
        "    \"\"\"\n",
        "    Reads the blind SST test set, which just consists of unlabeled sentences. Note that all words have been lowercased.\n",
        "    :param infile: path to the file to read\n",
        "    :return: list of tokenized sentences (list of list of strings)\n",
        "    \"\"\"\n",
        "    f = open(infile, encoding='utf-8')\n",
        "    exs = []\n",
        "    for line in f:\n",
        "        if len(line.strip()) > 0:\n",
        "            line = line.strip()\n",
        "            words = line.lower().split(\" \")\n",
        "            exs.append(SentimentExample(words, label=-1)) # pseudo label -1\n",
        "    return exs\n",
        "\n",
        "\n",
        "def write_sentiment_examples(exs: List[SentimentExample], outfile: str):\n",
        "    \"\"\"\n",
        "    Writes sentiment examples to an output file with one example per line, the predicted label followed by the example.\n",
        "    Note that what gets written out is tokenized.\n",
        "    :param exs: the list of SentimentExamples to write\n",
        "    :param outfile: out path\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    o = open(outfile, 'w')\n",
        "    for ex in exs:\n",
        "        o.write(repr(ex.label) + \"\\t\" + \" \".join([word for word in ex.words]) + \"\\n\")\n",
        "    o.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbC_dN5hvVEn"
      },
      "source": [
        "Now, load the training, dev, and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mUBkSCMpvVEo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6920 / 872 / 1821 train/dev/test examples\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = \"data\"\n",
        "# TODO: In case you are using Colab, uncommenting the following few lines of code\n",
        "#   to mount your Google Drive; refer to Assignment 2 for guide.\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# DATA_PATH = \"/content/drive/My Drive/CS478/data\"\n",
        "\n",
        "\n",
        "# Specify the data paths\n",
        "train_path = os.path.join(DATA_PATH, \"train.txt\")\n",
        "dev_path = os.path.join(DATA_PATH, \"dev.txt\")\n",
        "blind_test_path = os.path.join(DATA_PATH, \"test-blind.txt\") # blind test\n",
        "\n",
        "# Load train, dev, and test exs and index the words.\n",
        "train_exs = read_sentiment_examples(train_path)\n",
        "dev_exs = read_sentiment_examples(dev_path)\n",
        "test_exs_words_only = read_blind_sst_examples(blind_test_path)\n",
        "print(repr(len(train_exs)) + \" / \" + repr(len(dev_exs)) + \" / \" + repr(len(test_exs_words_only)) + \" train/dev/test examples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5kIyCD1vVEo"
      },
      "source": [
        "Let's see what's inside `train_exs`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twIiay6MvVEo"
      },
      "source": [
        "First, the number of examples (i.e., annotated sentences with sentiment labels) contained in `train_exs`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TLJIMWqqvVEo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6920"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_exs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_JSgFpDvVEo"
      },
      "source": [
        "Each data example can be assessed using index, e.g., accessing the first data example in train_exs, you can run `train_exs[0]`. Let's save it as `train_example_0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1OXU1l9ZvVEo"
      },
      "outputs": [],
      "source": [
        "train_example_0 = train_exs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF4f71RLvVEo"
      },
      "source": [
        "Each data example has a type:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Fjugf_PwvVEo"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "__main__.SentimentExample"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(train_example_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8gwVld4vVEo"
      },
      "source": [
        "In our implementation, each data example is an instance of our defined `SentimentExample` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi3laLxpvVEo"
      },
      "source": [
        "You can access its word tokens (**NOTE: when loading the dataset, we have already performed the sentence tokenization. There's NO NEED to do further preprocessing.**):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "02S8PXkovVEo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['the', 'rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'century', \"'s\", 'new', '``', 'conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'arnold', 'schwarzenegger', ',', 'jean-claud', 'van', 'damme', 'or', 'steven', 'segal', '.']\n"
          ]
        }
      ],
      "source": [
        "print(train_example_0.words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X22hMYkUvVEo"
      },
      "source": [
        "Similarly, you can access its sentiment label (1 for positive and 0 for negative):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CGJYPYsFvVEo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "print(train_example_0.label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuYZVuQdvVEo"
      },
      "source": [
        "Note that at this moment, we haven't created the vocabulary. Therefore, the `word_indices` of each example is still `None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "V6MGs_isvVEo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "print(train_example_0.word_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovr_v_hcvVEp"
      },
      "source": [
        "Now, let's create a vocabulary based on the training set `train_exs`. Similarly as our Assignment 2, we will keep only words with **more than 2** occurrences in our vocabulary, while converting others into a special `UNK` token.\n",
        "\n",
        "When working with neural nets, we will also add a `PAD` token for padding a batch of data examples.\n",
        "\n",
        "<font color='blue'>YOUR TASK: Please complete the following code block and for preparing the vocabulary.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RG876j7QvVEp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of words in the vocabulary: 485\n"
          ]
        }
      ],
      "source": [
        "# TODO: complete the code for creating a list called `vocab`, which contains a list of distinct words as the vocabulary.\n",
        "# As instructed, we only consider words occurring more than twice in the vocabulary.\n",
        "import nltk\n",
        "from collections import Counter\n",
        "vocab = []\n",
        "assert isinstance(vocab, list)\n",
        "for item in train_exs:\n",
        "    word_count = Counter(item.words)\n",
        "    for word in item.words:\n",
        "        if word_count[word] >= 2 and word not in vocab:\n",
        "            vocab.append(word)\n",
        "# Now add the special tokens PAD and UNK\n",
        "vocab = [\"PAD\", \"UNK\"] + vocab\n",
        "PAD_IDX = 0\n",
        "UNK_IDX = 1\n",
        "# Show the vocabulary size:\n",
        "print(\"Number of words in the vocabulary:\", len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH0vybi6vVEp"
      },
      "source": [
        "We then index the training, dev, and test set using this vocabulary.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "08BUm8_3vVEv"
      },
      "outputs": [],
      "source": [
        "indexing_sentiment_examples(train_exs, vocabulary=vocab, UNK_idx=UNK_IDX)\n",
        "indexing_sentiment_examples(dev_exs, vocabulary=vocab, UNK_idx=UNK_IDX)\n",
        "indexing_sentiment_examples(test_exs_words_only, vocabulary=vocab, UNK_idx=UNK_IDX)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT0aqWfMvVEv"
      },
      "source": [
        "If you check the `word_indices` of each data example now, you should see non-empty contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "dLuHFytwvVEv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The first example in the training set is indexed as: \n",
            " [2, 1, 39, 1, 3, 32, 2, 1, 1, 4, 121, 11, 1, 12, 13, 34, 55, 4, 1, 3, 73, 6, 1, 278, 1, 160, 1, 1, 9, 1, 1, 1, 91, 1, 1, 436]\n"
          ]
        }
      ],
      "source": [
        "print(\"The first example in the training set is indexed as:\", \"\\n\", train_example_0.word_indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MJVYZxyvVEv"
      },
      "source": [
        "**Step 2:** As we said in class, GPUs accelerate the computing via matrix operations. One important concept here is \"data batch\", i.e., instead of looking at one data example (i.e., one sentence) at a time, we look at a \"batch\" of examples. As such, all the calculation, such as loss computation, can be done via matrix operations.\n",
        "\n",
        "To this end, we will define a class called `SentimentExampleBatchIterator` for loading a batch of data from the given dataset.\n",
        "\n",
        "<font color='blue'>YOUR TASK: Please complete the following code block and implement the correct padding for loading a batch of examples.</font>\n",
        "\n",
        "For example, given\n",
        "```\n",
        "batch_exs = [\n",
        "    SentimentExample(words=[\"I\", \"feel\", \"happy\"], label=1, word_indices=[2,3,4]),\n",
        "    SentimentExample(words=[\"I\", \"feel\", \"sad\"], label=0, word_indices=[2,3,5]),\n",
        "    SentimentExample(words=[\"This\", \"movie\", \"is\", \"interesting\"], label=1, word_indices=[6,7,8,9])\n",
        "]\n",
        "```\n",
        "your code should generate (reminder: `0` in `batch_inputs` is the PAD index)\n",
        "```\n",
        "batch_inputs =\n",
        "    [[2, 3, 4, 0],\n",
        "    [2, 3, 5, 0],\n",
        "    [6, 7, 8, 9]],\n",
        "batch_lengths = [3, 3, 4],\n",
        "batch_labels = [1, 0, 1].\n",
        "```\n",
        "\n",
        "Tip: since the indexed SentimentExample object already has the indices saved in `word_indices`,\n",
        "what you need to do is to get them into one matrix (batch_inputs) and add PAD when necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "GGZyyYezvVEv"
      },
      "outputs": [],
      "source": [
        "from types import NotImplementedType\n",
        "class SentimentExampleBatchIterator:\n",
        "    \"\"\"\n",
        "    A batch iterator which will produce the next batch indexed data.\n",
        "\n",
        "    Attributes:\n",
        "        data: a list of SentimentExample objects, which is the source data input\n",
        "        batch_size: an integer number indicating the number of examples in each batch\n",
        "        PAD_idx: the index of PAD in the vocabulary\n",
        "        shuffle: whether to shuffle the data (should set to True only for training)\n",
        "    \"\"\"\n",
        "    def __init__(self, data: List[SentimentExample], batch_size: int, PAD_idx: int, shuffle: bool=True):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "        self.PAD_idx = PAD_idx\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        self._indices = None\n",
        "        self._cur_idx = None\n",
        "\n",
        "    def refresh(self):\n",
        "        self._indices = list(range(len(self.data)))\n",
        "        if self.shuffle:\n",
        "            random.shuffle(self._indices)\n",
        "        self._cur_idx = 0\n",
        "\n",
        "    def get_next_batch(self):\n",
        "        if self._cur_idx < len(self.data): # loop over the dataset\n",
        "            st_idx = self._cur_idx\n",
        "            if self._cur_idx + self.batch_size > len(self.data) - 1:\n",
        "                ed_idx = len(self.data)\n",
        "            else:\n",
        "                ed_idx = self._cur_idx + self.batch_size\n",
        "            self._cur_idx = ed_idx # update\n",
        "            # retrieve a batch of SentimentExample data\n",
        "            batch_exs = [self.data[self._indices[_idx]] for _idx in range(st_idx, ed_idx)]\n",
        "            \n",
        "            # TODO: implement the batching process, which returns batch_inputs, batch_lengths, and batch_labels\n",
        "            batch_lengths = [len(words.word_indices) for words in batch_exs]\n",
        "            max_len = max(batch_lengths)\n",
        "            batch_inputs = [words.word_indices + [self.PAD_idx] * (max_len - len(words.word_indices)) for words in batch_exs]\n",
        "            batch_labels = [words.label for words in batch_exs]\n",
        "            return (torch.tensor(batch_inputs), torch.tensor(batch_lengths), torch.tensor(batch_labels))\n",
        "        \n",
        "        else:\n",
        "            return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtRB875KvVEv"
      },
      "source": [
        "To test this batch iterator, run the following code and see if you can successfully load in the first four sentences in the training set into two batches, each containing 2 examples.\n",
        "\n",
        "(Note that this is not the actual batch_size we will use in experiment; we only do this for a sanity check.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZIC9m1ntvVEv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 0:\n",
            "tensor([[  2,   1,  39,   1,   3,  32,   2,   1,   1,   4, 121,  11,   1,  12,\n",
            "          13,  34,  55,   4,   1,   3,  73,   6,   1, 278,   1, 160,   1,   1,\n",
            "           9,   1,   1,   1,  91,   1,   1, 436,   0],\n",
            "        [  2,   1,   1,   1,   5,  11,   2,   1,   5,   2,   1,  12,   1,  39,\n",
            "          17,   1,  34,   6,   1,   5,   1, 175,  57,   1,   1,   1,   1,   1,\n",
            "           4,   1, 461,   5,   1,   1,   4,   1, 436]])\n",
            "tensor([36, 37])\n",
            "tensor([1, 1])\n",
            "----------\n",
            "Batch 1:\n",
            "tensor([[  1,   1,   1,   1,   6,   1,   5,   1,   7,   6,   8,   1,   1,   9,\n",
            "           6,   8,  48,   1,   1,   3,   2,  44,   7, 120,   2, 480,   1,   1,\n",
            "           1,   2,   1,   9,   1,   9,   1,   5,   2, 238, 436],\n",
            "        [182,   2,   1,  39, 273,   1,   1, 436,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])\n",
            "tensor([39,  8])\n",
            "tensor([1, 1])\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "toy_batch_iterator = SentimentExampleBatchIterator(train_exs[:4], batch_size=2, PAD_idx=0, shuffle=False) # hard-coded batch size and PAD_idx\n",
        "toy_batch_iterator.refresh()\n",
        "\n",
        "batch_count = 0\n",
        "batch_data = toy_batch_iterator.get_next_batch()\n",
        "while batch_data is not None:\n",
        "    print(\"Batch %d:\" % batch_count)\n",
        "    batch_inputs, batch_lengths, batch_labels = batch_data\n",
        "    # project to device\n",
        "    print(batch_inputs)\n",
        "    print(batch_lengths)\n",
        "    print(batch_labels)\n",
        "    print(\"-\" * 10)\n",
        "\n",
        "    batch_count += 1\n",
        "    batch_data = toy_batch_iterator.get_next_batch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn97N1sxvVEv"
      },
      "source": [
        "**Up to now, you've completed loading the datasets! This is your Checkpoint 1.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAUue6dEvVEv"
      },
      "source": [
        "## Part 3: FFNN Model Construction (25 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-jF7JS1vVEv"
      },
      "source": [
        "In this part, we will define the FFNN-based sentiment classifier called `FeedForwardNeuralNetClassifier`.\n",
        "\n",
        "<font color='blue'>YOUR TASK: Please read the in-line comments and fill out the `TODO`s.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDWh8YLNvVEv"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNeuralNetClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    The Feed-Forward Neural Net sentiment classifier.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, emb_dim, n_hidden_units):\n",
        "        \"\"\"\n",
        "        In the __init__ function, you will define modules in FFNN.\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param emb_dim: dimension of the embedding vectors\n",
        "        :param n_hidden_units: dimension of the hidden units\n",
        "        \"\"\"\n",
        "        super(FeedForwardNeuralNetClassifier, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_dim = emb_dim\n",
        "        self.n_hidden_units = n_hidden_units\n",
        "\n",
        "        # TODO: implement a randomly initialized word embedding matrix using nn.Embedding\n",
        "        # It should have a size of `(vocab_size x emb_dim)`\n",
        "        self.word_embeddings = None\n",
        "\n",
        "        # TODO: implement the FFNN architecture using nn functions\n",
        "        self.classifier = None\n",
        "\n",
        "        # TODO: the loss function\n",
        "        self.loss = None\n",
        "\n",
        "    def forward(self, batch_inputs: torch.Tensor, batch_lengths: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        The forward function, which defines how FFNN should work when given a batch of inputs and their actual sent lengths (i.e., before PAD)\n",
        "        :param batch_inputs: a torch.Tensor object of size (n_examples, max_sent_length_in_this_batch), which is the *indexed* inputs\n",
        "        :param batch_lengths: a torch.Tensor object of size (n_examples), which describes the actual sentence length of each example (i.e., before PAD)\n",
        "        :return the logits of FFNN (i.e., the unnormalized hidden units before sigmoid) of shape (n_examples)\n",
        "        \"\"\"\n",
        "        # TODO: implement\n",
        "        logits = None\n",
        "        return logits\n",
        "\n",
        "    def batch_predict(self, batch_inputs: torch.Tensor, batch_lengths: torch.Tensor) -> List[int]:\n",
        "        \"\"\"\n",
        "        Make predictions for a batch of inputs. This function may directly invoke `forward` (which passes the input through FFNN and returns the output logits)\n",
        "\n",
        "        :param batch_inputs: a torch.Tensor object of size (n_examples, max_sent_length_in_this_batch), which is the *indexed* inputs\n",
        "        :param batch_lengths: a torch.Tensor object of size (n_examples), which describes the actual sentence length of each example (i.e., before PAD)\n",
        "        :return: a list of predicted classes for this batch of data, either 0 for negative class or 1 for positive class\n",
        "        \"\"\"\n",
        "        # TODO: implement\n",
        "        preds = None\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfxZK2pNvVEv"
      },
      "source": [
        "## Part 4: Sentiment classifier training and evaluation (15 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tiqXU-ZvVEw"
      },
      "source": [
        "In this part, we will start training the FFNN classifier and then evaluate it on the dev set.\n",
        "\n",
        "<font color='blue'>YOUR TASK: Please read the following code and fill out the `TODO`s.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CioDK7-ivVEw"
      },
      "source": [
        "We will first instantiate a FFNN classifier (a `FeedForwardNeuralNetClassifier` object) with embedding size 300 and hidden size 300:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yR17cMSvVEw"
      },
      "outputs": [],
      "source": [
        "# TODO: create the FFNN classifier\n",
        "model = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdhiV-EmvVEw"
      },
      "source": [
        "You can view the model architecture by \"printing\" the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCGs9ZSHvVEw"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acnVDVK0vVEw"
      },
      "source": [
        "Define the \"device\" (CPU or GPU) to run the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAj4KbwnvVEw"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idZm5qczvVEw"
      },
      "source": [
        "If you are running the notebook on the GPU machine, you should see `cuda:0` popped out. This means that there's an available GPU device in your environment.\n",
        "\n",
        "Next, define an Adam optimizer using `torch.optim.Adam`, setting learning rate to 0.001 and other configs by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2GC07QFvVEw"
      },
      "outputs": [],
      "source": [
        "# TODO: create the optimizer\n",
        "optimizer = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hYyLcNivVEw"
      },
      "source": [
        "Before the training, we still need to set up the evaluation, such that we can monitor the model performance on the dev set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-CrdTNEvVEw"
      },
      "outputs": [],
      "source": [
        "def evaluate(classifier, exs: List[SentimentExample], return_metrics: bool=False):\n",
        "    \"\"\"\n",
        "    Evaluates a given classifier on the given examples\n",
        "    :param classifier: classifier to evaluate\n",
        "    :param exs: the list of SentimentExamples to evaluate on\n",
        "    :param return_metrics: set to True if returning the stats\n",
        "    :return: None (but prints output)\n",
        "    \"\"\"\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    eval_batch_iterator = SentimentExampleBatchIterator(exs, batch_size=32, PAD_idx=0, shuffle=False) # hard-coded batch size and PAD_idx\n",
        "    eval_batch_iterator.refresh()\n",
        "    batch_data = eval_batch_iterator.get_next_batch()\n",
        "    while batch_data is not None:\n",
        "        batch_inputs, batch_lengths, batch_labels = batch_data\n",
        "        # project to device\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_lengths = batch_lengths.to(device)\n",
        "        all_labels += list(batch_labels)\n",
        "\n",
        "        preds = classifier.batch_predict(batch_inputs, batch_lengths=batch_lengths)\n",
        "        all_preds += list(preds)\n",
        "        batch_data = eval_batch_iterator.get_next_batch()\n",
        "\n",
        "    if return_metrics:\n",
        "        acc, prec, rec, f1 = calculate_metrics(all_labels, all_preds)\n",
        "        return acc, prec, rec, f1\n",
        "    else:\n",
        "        calculate_metrics(all_labels, all_preds, print_only=True)\n",
        "\n",
        "\n",
        "def calculate_metrics(golds: List[int], predictions: List[int], print_only: bool=False):\n",
        "    \"\"\"\n",
        "    Calculate evaluation statistics comparing golds and predictions, each of which is a sequence of 0/1 labels.\n",
        "    Returns accuracy, precision, recall, and F1.\n",
        "\n",
        "    :param golds: gold labels\n",
        "    :param predictions: pred labels\n",
        "    :param print_only: set to True if printing the stats without returns\n",
        "    :return: accuracy, precision, recall, and F1 (all floating numbers), or None (when print_only is True)\n",
        "    \"\"\"\n",
        "    num_correct = 0\n",
        "    num_pos_correct = 0\n",
        "    num_pred = 0\n",
        "    num_gold = 0\n",
        "    num_total = 0\n",
        "    if len(golds) != len(predictions):\n",
        "        raise Exception(\"Mismatched gold/pred lengths: %i / %i\" % (len(golds), len(predictions)))\n",
        "    for idx in range(0, len(golds)):\n",
        "        gold = golds[idx]\n",
        "        prediction = predictions[idx]\n",
        "        if prediction == gold:\n",
        "            num_correct += 1\n",
        "        if prediction == 1:\n",
        "            num_pred += 1\n",
        "        if gold == 1:\n",
        "            num_gold += 1\n",
        "        if prediction == 1 and gold == 1:\n",
        "            num_pos_correct += 1\n",
        "        num_total += 1\n",
        "    acc = float(num_correct) / num_total\n",
        "    prec = float(num_pos_correct) / num_pred if num_pred > 0 else 0.0\n",
        "    rec = float(num_pos_correct) / num_gold if num_gold > 0 else 0.0\n",
        "    f1 = 2 * prec * rec / (prec + rec) if prec > 0 and rec > 0 else 0.0\n",
        "\n",
        "    print(\"Accuracy: %i / %i = %f\" % (num_correct, num_total, acc))\n",
        "    print(\"Precision (fraction of predicted positives that are correct): %i / %i = %f\" % (num_pos_correct, num_pred, prec)\n",
        "          + \"; Recall (fraction of true positives predicted correctly): %i / %i = %f\" % (num_pos_correct, num_gold, rec)\n",
        "          + \"; F1 (harmonic mean of precision and recall): %f\" % f1)\n",
        "\n",
        "    if not print_only:\n",
        "        return acc, prec, rec, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDWeq7iPvVEw"
      },
      "source": [
        "Now, complete the following code for training the model for 10 epochs with batch size 32. At the end of each epoch we also evaluate the model on the held-out dev set.\n",
        "\n",
        "Note: Your training should finish within a reasonable time period (say, up to 10 minutes) even when using CPU. There could be variantions case by case. However, if you see your training takes a much longer time, it can be that 1) your implementation is buggy or 2) your implementation can be significantly optimized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDhexgARvVEw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "BATCH_SIZE=32\n",
        "N_EPOCHS=10\n",
        "\n",
        "# create a batch iterator for the training data\n",
        "batch_iterator = SentimentExampleBatchIterator(\n",
        "    train_exs, batch_size=BATCH_SIZE, PAD_idx=PAD_IDX, shuffle=True)\n",
        "\n",
        "# training\n",
        "best_epoch = -1\n",
        "best_acc = -1\n",
        "start_time = time.time()\n",
        "for epoch in range(N_EPOCHS):\n",
        "    print(\"Epoch %i\" % epoch)\n",
        "\n",
        "    batch_iterator.refresh() # initiate a new iterator for this epoch\n",
        "\n",
        "    model.train() # turn on the \"training mode\"\n",
        "    batch_loss = 0.0\n",
        "    batch_example_count = 0\n",
        "    batch_data = batch_iterator.get_next_batch()\n",
        "    while batch_data is not None:\n",
        "        batch_inputs, batch_lengths, batch_labels = batch_data\n",
        "        # project to the device\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_lengths = batch_lengths.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        # TODO: clean up the gradients for this batch\n",
        "\n",
        "        # TODO: call the model and get the loss\n",
        "        loss = None\n",
        "\n",
        "        # record the loss and number of examples, so we could report some stats\n",
        "        batch_example_count += len(batch_labels)\n",
        "        batch_loss += loss.item() * len(batch_labels)\n",
        "\n",
        "        # TODO: backpropagation\n",
        "\n",
        "        # get another batch\n",
        "        batch_data = batch_iterator.get_next_batch()\n",
        "\n",
        "    print(\"Avg loss: %.5f\" % (batch_loss / batch_example_count))\n",
        "\n",
        "    # evaluate on dev set\n",
        "    model.eval() # turn on the \"evaluation mode\"\n",
        "    acc, _, _, _ = evaluate(model, dev_exs, return_metrics=True)\n",
        "    if acc > best_acc:\n",
        "        best_acc = acc\n",
        "        best_epoch = epoch\n",
        "        print(\"Secure a new best accuracy %.3f in epoch %d!\" % (best_acc, best_epoch))\n",
        "\n",
        "        # Save the current best model parameters\n",
        "        print(\"Save the best model checkpoint as `best_model.ckpt`!\")\n",
        "        torch.save(model.state_dict(), \"best_model.ckpt\")\n",
        "\n",
        "    print(\"Time elapsed: %s\" % time.strftime(\"%Hh%Mm%Ss\", time.gmtime(time.time()-start_time)))\n",
        "    print(\"-\" * 10)\n",
        "\n",
        "print(\"End of training! The best accuracy %.3f was obtained in epoch %d.\" % (best_acc, best_epoch))\n",
        "# Load back the best checkpoint on dev set\n",
        "model.load_state_dict(torch.load(\"best_model.ckpt\", weights_only=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qKJgounvVEx"
      },
      "source": [
        "**If your code runs well up to this point -- Congrats! You've completed the model training.**\n",
        "\n",
        "The following code will evaluate your model on the blind test set, with results saved in the folder `data_to_submit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yznrlFcvVEx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "path = os.path.join(DATA_PATH, \"data_to_submit\")\n",
        "if not os.path.exists(path):\n",
        "    os.mkdir(path)\n",
        "\n",
        "all_preds = [] # save the prediction results\n",
        "\n",
        "# iterator to load the test set\n",
        "eval_batch_iterator = SentimentExampleBatchIterator(test_exs_words_only, batch_size=32, PAD_idx=PAD_IDX, shuffle=False)\n",
        "eval_batch_iterator.refresh()\n",
        "batch_data = eval_batch_iterator.get_next_batch()\n",
        "while batch_data is not None:\n",
        "    batch_inputs, batch_lengths, _ = batch_data\n",
        "    # project to device\n",
        "    batch_inputs = batch_inputs.to(device)\n",
        "    batch_lengths = batch_lengths.to(device)\n",
        "\n",
        "    preds = model.batch_predict(batch_inputs, batch_lengths=batch_lengths) # the `preds` shoud be a list of prediction labels\n",
        "    all_preds += preds # accumulate the labels\n",
        "    batch_data = eval_batch_iterator.get_next_batch()\n",
        "\n",
        "# write the predicted labels along with its original sentence\n",
        "test_output_path = os.path.join(DATA_PATH, \"data_to_submit/test-blind.output.txt\")\n",
        "test_exs_predicted = [SentimentExample(ex.words, all_preds[ex_idx]) for ex_idx, ex in enumerate(test_exs_words_only)]\n",
        "write_sentiment_examples(test_exs_predicted, test_output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X7q3YRJvVEx"
      },
      "source": [
        "You've completed Part 4! Please don't forget to submit the `test-blind.output.txt` file along with your code to Blackboard!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z6U_b0XvVEx"
      },
      "source": [
        "## Part 5: Exploration of FFNN hyper-parameters (20 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QGL9I2IvVEx"
      },
      "source": [
        "Q5.1: The previous part has implemented the FFNN with both the embedding size and the hidden size as 300. Does it make any difference with a smaller embedding or hidden size?\n",
        "\n",
        "<font color='blue'>YOUR TASK: Please change `emb_dim` and `n_hidden_units` to other numbers, re-do all the experiment steps (**make sure to create a new model and a new optimizer for every experiment**), and report the results and your findings.</font>\n",
        "\n",
        "Here, you will report, for each configuration, the model accuracy on dev set and the epoch when it achieves the best accuracy (which allows to keep track of the model convergence speed). For a fair comparison, all experiments here should use the same batch size 32 and max number of epochs 10.\n",
        "\n",
        "\n",
        "To report your results, please fill out the table In the PDF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xV-r8mKqvVEx"
      },
      "source": [
        "Q5.2: Similarly, conduct experiments with different learning rates (but still use the Adam optimizer). Same as before, for a fair comparison, all experiments in this table should adopt the same configurations (other than the learning rate). To fill out this table, you can use emb_dim = n_hidden_units = 300, batch_size = 32, and max number of epochs = 10 as before.\n",
        "\n",
        "<font color='blue'>YOUR TASK: Please change the Adam learning rate to other numbers, re-do all the experiment steps (**again, make sure you create a new model and a new optimizer**), and report the results and your findings.</font>\n",
        "\n",
        "To report your results, please fill out the table in the PDF.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpBsJCKG8mJX"
      },
      "source": [
        "Q5.3: What do you observe from these two experiments and why do you think the observation should happen?\n",
        "\n",
        "<font color='blue'>YOUR TASK: Describe your answer in the PDF.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHb3bh4_vVEx"
      },
      "source": [
        "## Part 6: Exploration of the Pre-trained GloVe Word Embedding (15 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Et-orJzvVEx"
      },
      "source": [
        "The previous parts initialized the word embedding matrix randomly. Does it make any difference if we use pre-trained word embeddings such as GloVe? In the last part of this assignment, we will explore the use of GloVe."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_Cut1XQvVEx"
      },
      "source": [
        "First, download the GloVe embeddings from https://nlp.stanford.edu/projects/glove/ and save it under the assignment folder. Choose the version `Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download): glove.42B.300d.zip`. This process will take time.\n",
        "\n",
        "**Skip the following code if you have already downloaded the file manually.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iMD0DeUvVEx"
      },
      "outputs": [],
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.42B.300d.zip # do not run it for multiple time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuiP00GkvVEx"
      },
      "source": [
        "Then extract the file `glove.42B.300d.txt` from the folder. The txt file contains word and embedding pairs.\n",
        "\n",
        "**Skip the following code if you have already extracted the file manually.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5_qoccX-vVEx"
      },
      "outputs": [],
      "source": [
        "!unzip -o glove.42B.300d.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dFsxW9HvVEx"
      },
      "source": [
        "The following function can help read in the vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imv9GBAmvVEy"
      },
      "outputs": [],
      "source": [
        "def read_glove_pretrained_embeddings(path_to_glove_txt: str, vocab: set):\n",
        "    vec_dim = 300\n",
        "    word2vec = {}\n",
        "    with open(path_to_glove_txt, \"r\") as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.strip()\n",
        "            space_idx = line.find(' ')\n",
        "            word = line[:space_idx]\n",
        "            if word in vocab: # only words in the vocab will be considered\n",
        "                vec = np.array(line[space_idx+1:].split()).astype(float)\n",
        "                word2vec[word] = vec\n",
        "\n",
        "    return word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04i1uaphvVEy"
      },
      "source": [
        "While the entire GloVe is very large (containing embeddings for 1.9M words), we do not need all of them. Rather, we only need embeddings for words appearing in our sentiment classification dataset (for others, they won't be needed anyways, right?:)).\n",
        "\n",
        "In Part 2 when we created the vocabulary, we included only words with more than 2 occurrences, because a neural net is not likely to learn anything useful when a word appears only once in the training corpus. However, with GloVe, we want to reconsider those infrequent words because GloVe likely has learned about their semantics from its pre-training corpus.\n",
        "\n",
        "Therefore, our second step is to re-create a vocabulary, covering all words in the training, dev, and test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6EyL3TPvVEy"
      },
      "outputs": [],
      "source": [
        "vocab_counter = Counter([word for ex in train_exs + dev_exs + test_exs_words_only for word in ex.words])\n",
        "vocab = [word for word, _ in vocab_counter.most_common()]\n",
        "vocab = [\"PAD\", \"UNK\"] + vocab\n",
        "print(\"Vocab size:\", len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiQ2xomWvVEy"
      },
      "source": [
        "Now, we will write code to extract a subset of the word embeddings for words coverred in this new vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffwTdYkBvVEy"
      },
      "outputs": [],
      "source": [
        "glove_word2vec = read_glove_pretrained_embeddings(\"glove.42B.300d.txt\", set(vocab))\n",
        "\n",
        "glove_word_embeddings = []\n",
        "for word in vocab:\n",
        "    if word in glove_word2vec:\n",
        "        glove_word_embeddings.append(glove_word2vec[word])\n",
        "    else:\n",
        "        glove_word_embeddings.append(np.zeros(300, dtype=float)) # zero vectors for PAD/UNK/words not covered by glove\n",
        "\n",
        "glove_word_embeddings = torch.Tensor(np.array(glove_word_embeddings)).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_8w1ZExvVEy"
      },
      "source": [
        "<font color='blue'>YOUR TASK: Please read the following code and fill out the `TODO` for initializing FFNN with `glove_word_embeddings`. Keep the embedding parameters tunable.</font>\n",
        "\n",
        "Hint: You can use `nn.Embedding.from_pretrained` from PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WyLjet8vVEy"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNeuralNetClassifierwGlove(FeedForwardNeuralNetClassifier):\n",
        "    def __init__(self, vocab_size, emb_dim, n_hidden_units):\n",
        "        super().__init__(vocab_size, emb_dim, n_hidden_units)\n",
        "\n",
        "        # TODO: implement the use of pre-trained word embeddings\n",
        "        self.word_embeddings = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKzAJj7rvVEy"
      },
      "source": [
        "The above code created a child class of `FeedForwardNeuralNetClassifier` with the only difference lying in the initialization of the word embeddings. To run experiments using this new classifier, we re-initialize the `model` as an instance of this new classifier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--yNVN6CvVEy"
      },
      "outputs": [],
      "source": [
        "model = FeedForwardNeuralNetClassifierwGlove(vocab_size=len(vocab), emb_dim=300, n_hidden_units=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-EL2wxNvVEy"
      },
      "source": [
        "Then, please rerun the steps in Part 4 for model training (learning rate 0.001) and evaluation. Note that this training will take a longer time (in my record, 3-4minutes) as the embedding matrix is now much larger than before. If you have not optimized your model implementation code, better do it before this training.\n",
        "\n",
        "Report the new accuracy and the epoch giving the best accuracy in the PDF.\n",
        "\n",
        "What do you observe and why could it happen?\n",
        "\n",
        "<font color='blue'>Describe your findings and give your answer in the PDF.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "madgqPtFvVEy"
      },
      "source": [
        "## You have completed this assignment!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
